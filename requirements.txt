# Core deep learning
torch==2.0.1
transformers==4.35.2
datasets==2.14.5

# Scientific computing
numpy==1.24.3
scipy==1.10.1
pandas==1.5.3
scikit-learn==1.3.0

# Data handling
h5py==3.9.0
pyyaml==6.0.1

# Bioinformatics
biopython==1.81

# Utilities
tqdm==4.66.1
einops==0.7.0

# Visualization
matplotlib==3.7.2
seaborn==0.12.2

# Experiment tracking
wandb==0.15.12

# Development tools
jupyter==1.0.0
ipykernel==6.25.2
pytest==7.4.3
black==23.10.1
flake8==6.1.0
mypy==1.6.1

# Optional accelerators (uncomment if available)
# flash-attn==2.3.3  # For efficient attention (requires CUDA 11.6+ and SM 7.5+)
# triton==2.1.0  # Required for flash-attn
# apex==0.1  # For mixed precision training (alternative to PyTorch AMP)

# Installation instructions for Flash Attention:
# pip install packaging ninja
# pip install flash-attn --no-build-isolation